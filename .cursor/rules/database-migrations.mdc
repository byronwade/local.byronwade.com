---
description: USE WHEN creating, modifying, or discussing database migrations, schema changes, or table modifications
---

# Database Migration & Schema Evolution Rules

## üöÄ Migration Strategy: Zero-Downtime, Zero-Data-Loss

### Migration File Structure
```
scripts/migrations/
‚îú‚îÄ‚îÄ YYYY-MM-DD-HHMMSS-descriptive-name.sql
‚îú‚îÄ‚îÄ rollback/
‚îÇ   ‚îî‚îÄ‚îÄ YYYY-MM-DD-HHMMSS-descriptive-name-rollback.sql
‚îî‚îÄ‚îÄ README.md
```

## üìã Pre-Migration Checklist

### Required Steps Before ANY Schema Change:
1. **Backup Strategy**
   ```bash
   # Create timestamped backup
   pg_dump $DATABASE_URL > backups/$(date +%Y%m%d_%H%M%S)_pre_migration.sql
   ```

2. **Impact Assessment**
   - Identify affected tables and relationships
   - Estimate migration time and data volume
   - Check for dependent queries and indexes
   - Validate business logic dependencies

3. **Rollback Plan**
   - Create rollback script BEFORE applying migration
   - Test rollback on staging environment
   - Document rollback procedure and timing

### Migration Safety Patterns

#### ‚úÖ Safe Operations (Can be done online)
```sql
-- Adding nullable columns
ALTER TABLE businesses ADD COLUMN new_field TEXT;

-- Adding indexes concurrently
CREATE INDEX CONCURRENTLY idx_business_new_field ON businesses(new_field);

-- Adding check constraints (after data validation)
ALTER TABLE businesses ADD CONSTRAINT check_status 
CHECK (status IN ('active', 'inactive', 'pending'));
```

#### ‚ö†Ô∏è Risky Operations (Require careful planning)
```sql
-- Dropping columns (use multi-step approach)
-- Step 1: Stop using column in application
-- Step 2: Deploy without column usage
-- Step 3: Drop column in separate migration

-- Changing column types (requires data conversion)
-- Step 1: Add new column with new type
-- Step 2: Migrate data gradually
-- Step 3: Update application to use new column
-- Step 4: Drop old column
```

## üîÑ Multi-Step Migration Strategy

### For Complex Changes (Column Type, Constraints, etc.)
```sql
-- Example: Changing user_role from TEXT to ENUM

-- Migration 1: Add new column
ALTER TABLE users ADD COLUMN user_role_new user_role_enum;

-- Migration 2: Populate new column (can be done gradually)
UPDATE users SET user_role_new = user_role::user_role_enum 
WHERE user_role_new IS NULL LIMIT 1000;

-- Migration 3: Update application to use new column
-- (Deploy application code)

-- Migration 4: Drop old column
ALTER TABLE users DROP COLUMN user_role;
ALTER TABLE users RENAME COLUMN user_role_new TO user_role;
```

## üìä Data Migration Patterns

### Large Dataset Migration
```sql
-- Use batching to avoid locks
DO $$
DECLARE
    batch_size INTEGER := 1000;
    affected_rows INTEGER;
BEGIN
    LOOP
        UPDATE businesses 
        SET updated_field = new_value 
        WHERE old_condition 
        AND id IN (
            SELECT id FROM businesses 
            WHERE old_condition 
            LIMIT batch_size
        );
        
        GET DIAGNOSTICS affected_rows = ROW_COUNT;
        EXIT WHEN affected_rows = 0;
        
        -- Allow other operations between batches
        PERFORM pg_sleep(0.1);
    END LOOP;
END $$;
```

### Data Cleanup During Migration
```sql
-- Remove orphaned records during migration
DELETE FROM business_analytics ba
WHERE NOT EXISTS (
    SELECT 1 FROM businesses b 
    WHERE b.id = ba.business_id
);

-- Log cleanup results
INSERT INTO migration_logs (migration_name, action, affected_rows, timestamp)
VALUES ('cleanup_orphaned_analytics', 'DELETE', @@ROW_COUNT, NOW());
```

## üß™ Testing Strategy

### Required Testing Steps
1. **Schema Validation**
   ```bash
   # Verify schema completeness after migration
   node scripts/verify-schema.js
   ```

2. **Data Integrity Checks**
   ```sql
   -- Check for orphaned records
   SELECT 'business_analytics' as table_name, COUNT(*) as orphaned_count
   FROM business_analytics ba
   WHERE NOT EXISTS (SELECT 1 FROM businesses b WHERE b.id = ba.business_id)
   
   UNION ALL
   
   SELECT 'user_analytics', COUNT(*)
   FROM user_analytics ua
   WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = ua.user_id);
   ```

3. **Performance Impact**
   ```sql
   -- Analyze query performance after migration
   ANALYZE businesses;
   EXPLAIN ANALYZE SELECT * FROM businesses WHERE new_indexed_field = 'value';
   ```

## üîß Migration Templates

### New Table Template
```sql
-- Template for creating new tables
CREATE TABLE new_feature_table (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Feature-specific columns
    name TEXT NOT NULL,
    description TEXT,
    status TEXT DEFAULT 'active',
    
    -- Relationships
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    business_id UUID REFERENCES businesses(id) ON DELETE SET NULL,
    
    -- Metadata
    metadata JSONB DEFAULT '{}',
    
    -- Constraints
    CONSTRAINT valid_status CHECK (status IN ('active', 'inactive', 'archived'))
);

-- Indexes
CREATE INDEX idx_new_feature_table_user_id ON new_feature_table(user_id);
CREATE INDEX idx_new_feature_table_status ON new_feature_table(status);
CREATE INDEX idx_new_feature_table_created_at ON new_feature_table(created_at);

-- Trigger for updated_at
CREATE TRIGGER update_new_feature_table_updated_at
    BEFORE UPDATE ON new_feature_table
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Comments
COMMENT ON TABLE new_feature_table IS 'Description of what this table stores';
COMMENT ON COLUMN new_feature_table.metadata IS 'Additional flexible data storage';
```

### Index Addition Template
```sql
-- Template for adding indexes safely
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_table_column 
ON table_name(column_name);

-- Check index creation status
SELECT schemaname, tablename, indexname, indexdef 
FROM pg_indexes 
WHERE indexname = 'idx_table_column';
```

## üìà Migration Monitoring

### Track Migration Performance
```sql
-- Log migration execution time
CREATE TABLE migration_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    migration_name TEXT NOT NULL,
    action TEXT NOT NULL,
    affected_rows INTEGER,
    execution_time_ms INTEGER,
    timestamp TIMESTAMPTZ DEFAULT NOW(),
    success BOOLEAN DEFAULT TRUE,
    error_message TEXT
);
```

### Post-Migration Verification
```bash
#!/bin/bash
# Post-migration verification script

echo "üîç Running post-migration verification..."

# 1. Schema completeness
echo "‚úÖ Checking schema completeness..."
node scripts/verify-schema.js

# 2. Data integrity
echo "‚úÖ Checking data integrity..."
psql $DATABASE_URL -f scripts/check-data-integrity.sql

# 3. Performance benchmarks
echo "‚úÖ Running performance benchmarks..."
node scripts/performance-benchmark.js

# 4. Application health check
echo "‚úÖ Testing application endpoints..."
npm run test:integration

echo "üéâ Migration verification complete!"
```

## üö® Emergency Rollback Procedures

### Immediate Rollback Steps
1. **Stop Application Traffic**
   ```bash
   # Scale down application instances
   vercel --prod --scale 0
   ```

2. **Execute Rollback**
   ```bash
   # Run rollback script
   psql $DATABASE_URL -f scripts/rollback/migration-rollback.sql
   ```

3. **Verify Rollback**
   ```bash
   # Verify system state
   node scripts/verify-schema.js
   npm run test:integration
   ```

4. **Restore Traffic**
   ```bash
   # Scale up application
   vercel --prod --scale 1
   ```

## üí° Best Practices Summary

### DO:
- Always create rollback scripts first
- Test migrations on staging with production-sized data
- Use batching for large data migrations
- Monitor query performance impact
- Document breaking changes clearly
- Use `CONCURRENTLY` for index creation
- Implement gradual rollouts for risky changes

### DON'T:
- Run migrations without backups
- Change column types directly in production
- Add NOT NULL constraints without default values
- Drop tables/columns without multi-step approach
- Ignore query performance implications
- Rush migrations without proper testing

**Remember: Good migrations are boring migrations. Complex changes should be broken into simple, reversible steps.**