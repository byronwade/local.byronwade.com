---
description: USE WHEN performing data cleanup, archiving old data, removing unused records, or implementing data lifecycle management
---

# Data Cleanup & Lifecycle Management Rules

## üßπ Zero-Tolerance Policy for Data Bloat

**Every piece of data must have a purpose, lifecycle, and cleanup strategy.**

## üìä Data Lifecycle Categories

### Category 1: Transactional Data (Keep Indefinitely)
```sql
-- Core business data - never auto-delete
TABLES: users, businesses, reviews, subscriptions, payments
RETENTION: Permanent (soft delete only)
CLEANUP: Archive after 7+ years, but keep metadata
```

### Category 2: Analytics Data (Time-Limited)
```sql
-- Analytics and metrics - aggressive cleanup
TABLES: user_analytics, business_analytics, search_analytics, business_photo_analytics
RETENTION: 24 months detailed, 5 years aggregated
CLEANUP: Monthly archiving, quarterly aggregation
```

### Category 3: Activity Logs (Short-Term)
```sql
-- User activities and logs - shortest retention
TABLES: user_activities, local_hub_activities, audit_logs
RETENTION: 12 months detailed, 24 months summary
CLEANUP: Weekly cleanup, monthly archiving
```

### Category 4: Temporary Data (Immediate Cleanup)
```sql
-- Sessions, tokens, temporary files
TABLES: user_sessions, auth_tokens, temporary_uploads, password_resets
RETENTION: Hours to days
CLEANUP: Daily or real-time expiration
```

## üîÑ Automated Cleanup Jobs

### Daily Cleanup Script Template
```javascript
// scripts/daily-cleanup.js
import { supabase } from '../lib/supabase/client.js';
import { logger } from '../lib/utils/logger.js';

export async function dailyCleanup() {
  const startTime = performance.now();
  let cleanupStats = {};

  try {
    // 1. Clean expired sessions
    const { count: expiredSessions } = await supabase
      .from('user_sessions')
      .delete()
      .lt('expires_at', new Date().toISOString());
    
    cleanupStats.expiredSessions = expiredSessions;

    // 2. Clean expired password reset tokens
    const { count: expiredTokens } = await supabase
      .from('password_reset_tokens')
      .delete()
      .lt('expires_at', new Date().toISOString());
    
    cleanupStats.expiredTokens = expiredTokens;

    // 3. Clean temporary file uploads (older than 24 hours)
    const yesterday = new Date(Date.now() - 24 * 60 * 60 * 1000);
    const { count: tempFiles } = await supabase
      .from('temporary_uploads')
      .delete()
      .lt('created_at', yesterday.toISOString());
    
    cleanupStats.tempFiles = tempFiles;

    // 4. Clean orphaned records
    await cleanupOrphanedRecords();

    const duration = performance.now() - startTime;
    logger.info('Daily cleanup completed', { 
      cleanupStats, 
      duration: `${duration.toFixed(2)}ms` 
    });

  } catch (error) {
    logger.error('Daily cleanup failed', error);
    throw error;
  }
}

async function cleanupOrphanedRecords() {
  // Clean orphaned business analytics
  await supabase.rpc('cleanup_orphaned_business_analytics');
  
  // Clean orphaned user activities
  await supabase.rpc('cleanup_orphaned_user_activities');
  
  // Clean orphaned local hub data
  await supabase.rpc('cleanup_orphaned_local_hub_data');
}
```

### Weekly Cleanup Script Template
```javascript
// scripts/weekly-cleanup.js
export async function weeklyCleanup() {
  // 1. Archive old activities (older than 12 months)
  await archiveOldActivities();
  
  // 2. Cleanup inactive user accounts (no login for 2+ years)
  await cleanupInactiveUsers();
  
  // 3. Optimize database performance
  await optimizeDatabase();
  
  // 4. Generate cleanup report
  await generateCleanupReport();
}

async function archiveOldActivities() {
  const twelveMonthsAgo = new Date();
  twelveMonthsAgo.setMonth(twelveMonthsAgo.getMonth() - 12);

  // Archive user activities
  const { data: oldActivities } = await supabase
    .from('user_activities')
    .select('*')
    .lt('created_at', twelveMonthsAgo.toISOString());

  if (oldActivities?.length > 0) {
    // Move to archive table
    await supabase.from('user_activities_archive').insert(oldActivities);
    
    // Delete from main table
    await supabase
      .from('user_activities')
      .delete()
      .lt('created_at', twelveMonthsAgo.toISOString());
    
    logger.info(`Archived ${oldActivities.length} old user activities`);
  }
}
```

### Monthly Cleanup Script Template
```javascript
// scripts/monthly-cleanup.js
export async function monthlyCleanup() {
  // 1. Archive old analytics data (older than 24 months)
  await archiveOldAnalytics();
  
  // 2. Aggregate archived data into summary tables
  await aggregateArchivedData();
  
  // 3. Clean up soft-deleted records (older than 90 days)
  await cleanupSoftDeletedRecords();
  
  // 4. Optimize storage and indexes
  await optimizeStorage();
}

async function archiveOldAnalytics() {
  const cutoffDate = new Date();
  cutoffDate.setMonth(cutoffDate.getMonth() - 24);

  const analyticsTypes = [
    'user_analytics',
    'business_analytics', 
    'search_analytics',
    'business_photo_analytics',
    'local_hub_analytics'
  ];

  for (const table of analyticsTypes) {
    const { data: oldData } = await supabase
      .from(table)
      .select('*')
      .lt('date', cutoffDate.toISOString().split('T')[0]);

    if (oldData?.length > 0) {
      // Archive data
      await supabase.from(`${table}_archive`).insert(oldData);
      
      // Delete from main table
      await supabase
        .from(table)
        .delete()
        .lt('date', cutoffDate.toISOString().split('T')[0]);
      
      logger.info(`Archived ${oldData.length} records from ${table}`);
    }
  }
}
```

## üóÑÔ∏è Data Archiving Strategy

### Archive Table Structure
```sql
-- Create archive tables with same structure but different indexes
CREATE TABLE user_analytics_archive (
    LIKE user_analytics INCLUDING ALL
);

-- Add archival metadata
ALTER TABLE user_analytics_archive 
ADD COLUMN archived_at TIMESTAMPTZ DEFAULT NOW(),
ADD COLUMN archive_reason TEXT DEFAULT 'age_based_archival';

-- Optimize for read-only access
CREATE INDEX idx_user_analytics_archive_date_user 
ON user_analytics_archive(date, user_id);

-- Partition by year for efficient queries
CREATE TABLE user_analytics_archive_2024 
PARTITION OF user_analytics_archive 
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

### Data Compression for Archives
```sql
-- Enable compression for archive tables
ALTER TABLE user_analytics_archive SET (
    toast_tuple_target = 128,
    fillfactor = 90
);

-- Vacuum and analyze for optimal storage
VACUUM (ANALYZE, VERBOSE) user_analytics_archive;
```

## üßΩ Cleanup Functions (PostgreSQL)

### Orphaned Record Cleanup
```sql
-- Function to clean orphaned business analytics
CREATE OR REPLACE FUNCTION cleanup_orphaned_business_analytics()
RETURNS TABLE(cleaned_count INTEGER) AS $$
DECLARE
    cleanup_count INTEGER;
BEGIN
    -- Delete orphaned business analytics
    DELETE FROM business_analytics ba
    WHERE NOT EXISTS (
        SELECT 1 FROM businesses b 
        WHERE b.id = ba.business_id
    );
    
    GET DIAGNOSTICS cleanup_count = ROW_COUNT;
    
    -- Log the cleanup
    INSERT INTO cleanup_logs (table_name, action, affected_rows, timestamp)
    VALUES ('business_analytics', 'cleanup_orphaned', cleanup_count, NOW());
    
    RETURN QUERY SELECT cleanup_count;
END;
$$ LANGUAGE plpgsql;
```

### Batch Deletion Function
```sql
-- Function for safe batch deletion
CREATE OR REPLACE FUNCTION batch_delete_old_data(
    table_name TEXT,
    date_column TEXT,
    cutoff_date DATE,
    batch_size INTEGER DEFAULT 1000
) RETURNS INTEGER AS $$
DECLARE
    total_deleted INTEGER := 0;
    batch_deleted INTEGER;
    sql_query TEXT;
BEGIN
    LOOP
        -- Construct dynamic SQL for batch deletion
        sql_query := format(
            'DELETE FROM %I WHERE %I < $1 AND ctid IN (
                SELECT ctid FROM %I WHERE %I < $1 LIMIT $2
            )',
            table_name, date_column, table_name, date_column
        );
        
        EXECUTE sql_query USING cutoff_date, batch_size;
        GET DIAGNOSTICS batch_deleted = ROW_COUNT;
        
        total_deleted := total_deleted + batch_deleted;
        
        -- Exit if no more rows to delete
        EXIT WHEN batch_deleted = 0;
        
        -- Small delay to reduce lock contention
        PERFORM pg_sleep(0.1);
    END LOOP;
    
    RETURN total_deleted;
END;
$$ LANGUAGE plpgsql;
```

## üìà Cleanup Monitoring & Reporting

### Cleanup Metrics Dashboard
```javascript
// Generate cleanup metrics for monitoring
export async function generateCleanupMetrics() {
  const metrics = {};

  // Database size trends
  const { data: dbSize } = await supabase.rpc('get_database_size');
  metrics.databaseSize = dbSize;

  // Table sizes and row counts
  const tables = [
    'user_analytics', 'business_analytics', 'user_activities',
    'local_hub_activities', 'user_sessions', 'temporary_uploads'
  ];

  for (const table of tables) {
    const { count } = await supabase.from(table).select('*', { count: 'exact', head: true });
    metrics[`${table}_count`] = count;
  }

  // Orphaned record counts
  metrics.orphanedRecords = await checkOrphanedRecords();

  // Archive effectiveness
  metrics.archiveStats = await getArchiveStats();

  return metrics;
}

async function checkOrphanedRecords() {
  const orphanedChecks = [
    {
      name: 'business_analytics_orphaned',
      query: `
        SELECT COUNT(*) FROM business_analytics ba
        WHERE NOT EXISTS (SELECT 1 FROM businesses b WHERE b.id = ba.business_id)
      `
    },
    {
      name: 'user_analytics_orphaned', 
      query: `
        SELECT COUNT(*) FROM user_analytics ua
        WHERE NOT EXISTS (SELECT 1 FROM users u WHERE u.id = ua.user_id)
      `
    }
  ];

  const results = {};
  for (const check of orphanedChecks) {
    const { data } = await supabase.rpc('execute_sql', { sql: check.query });
    results[check.name] = data?.[0]?.count || 0;
  }

  return results;
}
```

### Automated Cleanup Scheduling
```javascript
// Schedule cleanup jobs using cron-like syntax
export const cleanupSchedule = {
  // Every hour: Clean expired sessions
  '0 * * * *': () => cleanupExpiredSessions(),
  
  // Daily at 2 AM: Full daily cleanup
  '0 2 * * *': () => dailyCleanup(),
  
  // Weekly on Sunday at 3 AM: Archive old activities
  '0 3 * * 0': () => weeklyCleanup(),
  
  // Monthly on 1st at 4 AM: Archive analytics data
  '0 4 1 * *': () => monthlyCleanup(),
  
  // Quarterly: Full database optimization
  '0 5 1 1,4,7,10 *': () => quarterlyOptimization()
};
```

## üö® Data Cleanup Safety Rules

### Pre-Cleanup Validation
```javascript
export async function validateCleanupSafety(table, conditions) {
  // 1. Estimate affected records
  const { count } = await supabase
    .from(table)
    .select('*', { count: 'exact', head: true })
    .match(conditions);

  // 2. Safety checks
  if (count > 10000) {
    throw new Error(`Cleanup would affect ${count} records. Use batch processing.`);
  }

  // 3. Check for foreign key dependencies
  const dependencies = await checkForeignKeyDependencies(table);
  if (dependencies.length > 0) {
    logger.warn(`Table ${table} has dependencies:`, dependencies);
  }

  // 4. Create backup before large cleanups
  if (count > 1000) {
    await createTableBackup(table, conditions);
  }

  return { safeToCleanup: true, affectedRecords: count };
}
```

### Emergency Data Recovery
```sql
-- Emergency recovery procedures
CREATE TABLE data_recovery_log (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name TEXT NOT NULL,
    recovery_point TIMESTAMPTZ NOT NULL,
    backup_location TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Point-in-time recovery function
CREATE OR REPLACE FUNCTION create_recovery_point(table_name TEXT)
RETURNS TEXT AS $$
DECLARE
    backup_name TEXT;
BEGIN
    backup_name := table_name || '_backup_' || to_char(NOW(), 'YYYY_MM_DD_HH24_MI_SS');
    
    EXECUTE format('CREATE TABLE %I AS SELECT * FROM %I', backup_name, table_name);
    
    INSERT INTO data_recovery_log (table_name, recovery_point, backup_location)
    VALUES (table_name, NOW(), backup_name);
    
    RETURN backup_name;
END;
$$ LANGUAGE plpgsql;
```

## üí° Best Practices Summary

### DO:
- Always test cleanup scripts on staging first
- Create backups before large deletions
- Use batch processing for large datasets
- Monitor cleanup job performance
- Log all cleanup operations
- Implement gradual cleanup (not all at once)
- Set up alerts for cleanup failures

### DON'T:
- Delete data without archiving strategy
- Run cleanup without performance impact assessment
- Ignore foreign key constraints during cleanup
- Delete large amounts of data in single transaction
- Skip backup creation for risky operations
- Cleanup during peak usage hours

### Critical Metrics to Monitor:
- Database size growth rate
- Cleanup job execution time
- Orphaned record counts
- Archive table sizes
- Query performance impact

**Remember: Deleted data is gone forever. Archive first, cleanup second, optimize third.**