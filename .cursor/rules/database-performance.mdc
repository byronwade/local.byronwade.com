---
description: USE WHEN optimizing database performance, analyzing slow queries, adding indexes, or monitoring database metrics
---

# Database Performance Optimization Rules

## ‚ö° Performance-First Database Architecture

**Every query must be fast. Every table must be optimized. Every index must have a purpose.**

## üéØ Performance Targets

### Critical Performance Metrics
```javascript
const performanceTargets = {
  // Query Performance
  averageQueryTime: '<300ms',
  p95QueryTime: '<1000ms', 
  slowQueryThreshold: '1000ms',
  
  // Database Health
  indexHitRatio: '>95%',
  connectionPoolUsage: '<70%',
  tableBloat: '<20%',
  
  // Application Performance  
  apiResponseTime: '<200ms',
  pageLoadTime: '<3000ms',
  timeToFirstByte: '<500ms'
};
```

## üìä Query Performance Monitoring

### Performance Tracking Implementation
```javascript
// lib/utils/performanceMonitor.js
export class DatabasePerformanceMonitor {
  static trackQuery(operation, startTime, queryInfo) {
    const duration = performance.now() - startTime;
    
    // Log to monitoring system
    logger.performance({
      operation,
      duration,
      table: queryInfo.table,
      queryType: queryInfo.type,
      rowCount: queryInfo.rowCount,
      indexesUsed: queryInfo.indexes,
      timestamp: Date.now()
    });

    // Alert on slow queries
    if (duration > 1000) {
      logger.critical(`Slow query detected: ${operation} took ${duration.toFixed(2)}ms`, {
        query: queryInfo.sql,
        explain: queryInfo.explainPlan
      });
    }

    // Track performance trends
    this.updatePerformanceTrends(operation, duration);
  }

  static async analyzeSlowQueries() {
    // Get slow queries from pg_stat_statements
    const { data: slowQueries } = await supabase.rpc('get_slow_queries', {
      min_duration_ms: 1000,
      limit_count: 50
    });

    return slowQueries.map(query => ({
      query: query.query,
      avgDuration: query.mean_time,
      totalTime: query.total_time,
      calls: query.calls,
      recommendedOptimization: this.suggestOptimization(query)
    }));
  }
}
```

### Slow Query Analysis Function
```sql
-- Function to analyze slow queries
CREATE OR REPLACE FUNCTION get_slow_queries(
    min_duration_ms INTEGER DEFAULT 1000,
    limit_count INTEGER DEFAULT 20
) RETURNS TABLE (
    query TEXT,
    calls BIGINT,
    total_time DOUBLE PRECISION,
    mean_time DOUBLE PRECISION,
    rows BIGINT,
    hit_percent DOUBLE PRECISION
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        pg_stat_statements.query,
        pg_stat_statements.calls,
        pg_stat_statements.total_exec_time,
        pg_stat_statements.mean_exec_time,
        pg_stat_statements.rows,
        (pg_stat_statements.shared_blks_hit * 100.0 / 
         NULLIF(pg_stat_statements.shared_blks_hit + pg_stat_statements.shared_blks_read, 0)) as hit_percent
    FROM pg_stat_statements
    WHERE pg_stat_statements.mean_exec_time > min_duration_ms
    ORDER BY pg_stat_statements.mean_exec_time DESC
    LIMIT limit_count;
END;
$$ LANGUAGE plpgsql;
```

## üóÇÔ∏è Index Optimization Strategy

### Index Analysis & Recommendations
```sql
-- Function to identify missing indexes
CREATE OR REPLACE FUNCTION suggest_missing_indexes()
RETURNS TABLE (
    table_name TEXT,
    column_name TEXT,
    usage_count BIGINT,
    recommendation TEXT
) AS $$
BEGIN
    RETURN QUERY
    -- Analyze foreign key columns without indexes
    SELECT 
        tc.table_name::TEXT,
        kcu.column_name::TEXT,
        COALESCE(pg_stat_user_tables.seq_scan, 0) as usage_count,
        'Add index on foreign key column'::TEXT as recommendation
    FROM information_schema.table_constraints tc
    JOIN information_schema.key_column_usage kcu 
        ON tc.constraint_name = kcu.constraint_name
    LEFT JOIN pg_stat_user_tables 
        ON pg_stat_user_tables.relname = tc.table_name
    WHERE tc.constraint_type = 'FOREIGN KEY'
    AND NOT EXISTS (
        SELECT 1 FROM pg_indexes 
        WHERE tablename = tc.table_name 
        AND indexdef LIKE '%' || kcu.column_name || '%'
    );
END;
$$ LANGUAGE plpgsql;
```

### Index Usage Analysis
```sql
-- Function to identify unused indexes
CREATE OR REPLACE FUNCTION find_unused_indexes()
RETURNS TABLE (
    schemaname TEXT,
    tablename TEXT, 
    indexname TEXT,
    index_size TEXT,
    index_scans BIGINT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        pg_stat_user_indexes.schemaname::TEXT,
        pg_stat_user_indexes.relname::TEXT,
        pg_stat_user_indexes.indexrelname::TEXT,
        pg_size_pretty(pg_relation_size(pg_stat_user_indexes.indexrelid))::TEXT,
        pg_stat_user_indexes.idx_scan
    FROM pg_stat_user_indexes
    WHERE pg_stat_user_indexes.idx_scan < 10  -- Adjust threshold as needed
    AND pg_stat_user_indexes.schemaname = 'public'
    ORDER BY pg_relation_size(pg_stat_user_indexes.indexrelid) DESC;
END;
$$ LANGUAGE plpgsql;
```

## üöÄ Performance Optimization Techniques

### Query Optimization Patterns
```javascript
// Example: Optimized business search with proper indexing
export async function optimizedBusinessSearch(params) {
  const startTime = performance.now();
  
  // Use covering indexes and efficient filtering
  let query = supabase
    .from('businesses')
    .select(`
      id, name, rating, review_count, featured,
      business_categories!inner(category_id),
      business_analytics(views, calls)
    `)
    .eq('status', 'published')  // Use indexed status column first
    .eq('verified', true);      // Use indexed verified column

  // Apply location filter with spatial index
  if (params.bounds) {
    query = query.rpc('businesses_in_bounds', {
      north: params.bounds.north,
      south: params.bounds.south,
      east: params.bounds.east,
      west: params.bounds.west
    });
  }

  // Use text search index for queries
  if (params.query) {
    query = query.textSearch('search_vector', params.query, {
      type: 'websearch',
      config: 'english'
    });
  }

  // Efficient pagination
  const limit = Math.min(params.limit || 20, 100);
  const offset = params.offset || 0;
  query = query.range(offset, offset + limit - 1);

  const { data, error } = await query;

  // Track performance
  DatabasePerformanceMonitor.trackQuery('business_search', startTime, {
    table: 'businesses',
    type: 'SELECT',
    rowCount: data?.length || 0,
    hasLocationFilter: !!params.bounds,
    hasTextSearch: !!params.query
  });

  return { data, error };
}
```

### Efficient Aggregation Queries
```sql
-- Optimized analytics aggregation with proper indexing
CREATE OR REPLACE FUNCTION get_business_analytics_summary(
    business_id UUID,
    start_date DATE,
    end_date DATE
) RETURNS TABLE (
    total_views BIGINT,
    total_calls BIGINT,
    total_directions BIGINT,
    avg_daily_views NUMERIC,
    growth_rate NUMERIC
) AS $$
BEGIN
    RETURN QUERY
    WITH daily_stats AS (
        SELECT 
            date,
            views,
            calls,
            directions
        FROM business_analytics 
        WHERE business_analytics.business_id = get_business_analytics_summary.business_id
        AND date BETWEEN start_date AND end_date
    ),
    aggregated AS (
        SELECT 
            SUM(views) as total_views,
            SUM(calls) as total_calls, 
            SUM(directions) as total_directions,
            AVG(views) as avg_daily_views,
            COUNT(*) as days_count
        FROM daily_stats
    ),
    growth AS (
        SELECT 
            CASE 
                WHEN LAG(views) OVER (ORDER BY date) > 0 
                THEN ((views - LAG(views) OVER (ORDER BY date)) * 100.0 / LAG(views) OVER (ORDER BY date))
                ELSE 0 
            END as daily_growth
        FROM daily_stats
        ORDER BY date DESC
        LIMIT 7
    )
    SELECT 
        a.total_views,
        a.total_calls,
        a.total_directions,
        a.avg_daily_views,
        COALESCE(AVG(g.daily_growth), 0) as growth_rate
    FROM aggregated a
    CROSS JOIN growth g
    GROUP BY a.total_views, a.total_calls, a.total_directions, a.avg_daily_views;
END;
$$ LANGUAGE plpgsql;

-- Ensure proper indexing for the function
CREATE INDEX IF NOT EXISTS idx_business_analytics_business_date 
ON business_analytics(business_id, date) 
INCLUDE (views, calls, directions);
```

## üìà Database Health Monitoring

### Health Check Function
```sql
-- Comprehensive database health check
CREATE OR REPLACE FUNCTION database_health_check()
RETURNS TABLE (
    metric_name TEXT,
    current_value TEXT,
    status TEXT,
    recommendation TEXT
) AS $$
DECLARE
    db_size BIGINT;
    connection_count INTEGER;
    slow_query_count INTEGER;
    bloat_ratio NUMERIC;
    index_hit_ratio NUMERIC;
BEGIN
    -- Database size
    SELECT pg_database_size(current_database()) INTO db_size;
    
    -- Active connections
    SELECT count(*) FROM pg_stat_activity WHERE state = 'active' INTO connection_count;
    
    -- Slow queries (> 1 second)
    SELECT count(*) FROM pg_stat_statements 
    WHERE mean_exec_time > 1000 INTO slow_query_count;
    
    -- Index hit ratio
    SELECT (sum(idx_blks_hit) * 100.0 / NULLIF(sum(idx_blks_hit + idx_blks_read), 0))
    FROM pg_statio_user_indexes INTO index_hit_ratio;
    
    -- Return health metrics
    RETURN QUERY VALUES
    ('database_size', pg_size_pretty(db_size), 
     CASE WHEN db_size > 10737418240 THEN 'WARNING' ELSE 'OK' END,
     CASE WHEN db_size > 10737418240 THEN 'Consider archiving old data' ELSE 'Size is healthy' END),
    
    ('active_connections', connection_count::TEXT,
     CASE WHEN connection_count > 80 THEN 'WARNING' ELSE 'OK' END,
     CASE WHEN connection_count > 80 THEN 'Monitor connection pooling' ELSE 'Connection count is healthy' END),
    
    ('slow_queries', slow_query_count::TEXT,
     CASE WHEN slow_query_count > 10 THEN 'WARNING' ELSE 'OK' END,
     CASE WHEN slow_query_count > 10 THEN 'Optimize slow queries' ELSE 'Query performance is good' END),
    
    ('index_hit_ratio', ROUND(index_hit_ratio, 2)::TEXT || '%',
     CASE WHEN index_hit_ratio < 95 THEN 'WARNING' ELSE 'OK' END,
     CASE WHEN index_hit_ratio < 95 THEN 'Add missing indexes' ELSE 'Index usage is optimal' END);
END;
$$ LANGUAGE plpgsql;
```

### Performance Alerting System
```javascript
// scripts/performance-monitoring.js
export async function monitorDatabasePerformance() {
  const healthCheck = await supabase.rpc('database_health_check');
  const slowQueries = await supabase.rpc('get_slow_queries');
  const missingIndexes = await supabase.rpc('suggest_missing_indexes');
  
  // Check for critical issues
  const criticalIssues = healthCheck.data?.filter(metric => 
    metric.status === 'WARNING'
  );

  if (criticalIssues?.length > 0) {
    await sendPerformanceAlert({
      type: 'critical',
      issues: criticalIssues,
      slowQueries: slowQueries.data?.slice(0, 5),
      missingIndexes: missingIndexes.data?.slice(0, 3)
    });
  }

  // Log performance metrics
  logger.performance('Database health check completed', {
    healthMetrics: healthCheck.data,
    slowQueryCount: slowQueries.data?.length || 0,
    missingIndexCount: missingIndexes.data?.length || 0
  });

  return {
    health: healthCheck.data,
    slowQueries: slowQueries.data,
    missingIndexes: missingIndexes.data
  };
}

async function sendPerformanceAlert(alertData) {
  // Send to monitoring service (DataDog, New Relic, etc.)
  await fetch(process.env.MONITORING_WEBHOOK_URL, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      alert_type: 'database_performance',
      severity: alertData.type,
      timestamp: new Date().toISOString(),
      details: alertData
    })
  });
}
```

## üéõÔ∏è Query Optimization Tools

### EXPLAIN Plan Analysis
```javascript
// Automated query plan analysis
export async function analyzeQueryPlan(sql, params = []) {
  const { data: explainResult } = await supabase.rpc('explain_query', {
    query_sql: sql,
    query_params: params
  });

  const analysis = {
    estimatedCost: extractCost(explainResult),
    scanTypes: extractScanTypes(explainResult),
    indexUsage: extractIndexUsage(explainResult),
    recommendations: generateRecommendations(explainResult)
  };

  return analysis;
}

function generateRecommendations(explainPlan) {
  const recommendations = [];
  
  if (explainPlan.includes('Seq Scan')) {
    recommendations.push('Consider adding indexes for sequential scans');
  }
  
  if (explainPlan.includes('cost=') && extractCost(explainPlan) > 10000) {
    recommendations.push('High query cost detected - consider optimization');
  }
  
  if (explainPlan.includes('rows=') && extractRows(explainPlan) > 100000) {
    recommendations.push('Large result set - consider pagination or filtering');
  }

  return recommendations;
}
```

### Index Recommendation Engine
```sql
-- Advanced index recommendation based on query patterns
CREATE OR REPLACE FUNCTION recommend_indexes_for_table(table_name TEXT)
RETURNS TABLE (
    recommended_index TEXT,
    reason TEXT,
    estimated_benefit TEXT
) AS $$
DECLARE
    rec RECORD;
BEGIN
    -- Analyze WHERE clause patterns
    FOR rec IN
        SELECT 
            pss.query,
            pss.calls,
            pss.mean_exec_time,
            pss.total_exec_time
        FROM pg_stat_statements pss
        WHERE pss.query ILIKE '%FROM ' || table_name || '%'
        OR pss.query ILIKE '%JOIN ' || table_name || '%'
        ORDER BY pss.total_exec_time DESC
        LIMIT 20
    LOOP
        -- Extract potential index candidates from WHERE clauses
        -- This is a simplified example - real implementation would parse SQL
        IF rec.query ILIKE '%WHERE%created_at%' THEN
            RETURN QUERY VALUES 
            ('CREATE INDEX idx_' || table_name || '_created_at ON ' || table_name || '(created_at)',
             'Frequent date filtering detected',
             'Could reduce query time by ' || ROUND(rec.mean_exec_time * 0.7) || 'ms');
        END IF;
        
        IF rec.query ILIKE '%WHERE%user_id%' THEN
            RETURN QUERY VALUES 
            ('CREATE INDEX idx_' || table_name || '_user_id ON ' || table_name || '(user_id)',
             'Frequent user filtering detected', 
             'Could reduce query time by ' || ROUND(rec.mean_exec_time * 0.8) || 'ms');
        END IF;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
```

## üí° Performance Best Practices Summary

### Query Optimization Checklist:
- [ ] Use appropriate indexes for WHERE, ORDER BY, and JOIN clauses
- [ ] Avoid SELECT * in production queries  
- [ ] Use LIMIT for large result sets
- [ ] Implement proper pagination
- [ ] Use prepared statements to prevent SQL injection and improve performance
- [ ] Cache frequently accessed, slow-changing data
- [ ] Use batch operations for bulk inserts/updates

### Index Strategy:
- [ ] Index all foreign key columns
- [ ] Create composite indexes for multi-column WHERE clauses
- [ ] Use partial indexes for filtered queries
- [ ] Monitor index usage and remove unused indexes
- [ ] Consider covering indexes for read-heavy workloads

### Database Maintenance:
- [ ] Regular VACUUM and ANALYZE operations
- [ ] Monitor table bloat and reindex when necessary
- [ ] Archive old data to keep tables lean
- [ ] Monitor slow query log and optimize problematic queries
- [ ] Set up automated performance monitoring

### Critical Performance Alerts:
- [ ] Query execution time > 1000ms
- [ ] Index hit ratio < 95%
- [ ] Connection pool usage > 80%
- [ ] Database size growth > 50% monthly
- [ ] Table bloat > 20%

**Remember: Performance optimization is an ongoing process. Monitor, measure, and optimize continuously.**